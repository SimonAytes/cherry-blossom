{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f984ed5-9bfd-45e3-9302-1f067545ad5b",
   "metadata": {},
   "source": [
    "# Boulevard of broken analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f9b31-fd5b-4427-9515-3a01cbbb76ce",
   "metadata": {},
   "source": [
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41457ddb-22da-4758-9e67-523f5ef0f58f",
   "metadata": {},
   "source": [
    "Initial model R2 was ~86%. Post-outlier detection (defined as a row where more than 10% of members were outliers) saw a drop to between 45-65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0989a211-3a11-4b7b-b474-8d8378265c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def detect_outliers(data, contamination=0.05, random_state=None):\n",
    "    \"\"\"\n",
    "    Detect outliers/anomalies in each feature of the data using Isolation Forest algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The input data for outlier detection.\n",
    "        contamination (float, optional): The proportion of outliers/anomalies in the data.\n",
    "                                         Defaults to 0.05.\n",
    "        random_state (int, RandomState instance or None, optional): \n",
    "            Controls the random seed for reproducibility. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A DataFrame indicating whether each data point is an outlier/anomaly (1) or not (0) for each feature.\n",
    "    \"\"\"\n",
    "    # Initialize DataFrame to store outlier detection results\n",
    "    outlier_df = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = data.columns.tolist()\n",
    "    \n",
    "    # Initialize Isolation Forest model with feature names\n",
    "    model = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    \n",
    "    # Initialize list to store outlier predictions\n",
    "    outlier_columns = []\n",
    "    \n",
    "    # Detect outliers for each feature\n",
    "    for column in data.columns:\n",
    "        # Fit the model and predict outliers for the current feature\n",
    "        outliers = model.fit_predict(data[[column]].values)\n",
    "        \n",
    "        # Convert outliers numpy array to DataFrame\n",
    "        outlier_series = pd.Series(outliers, index=data.index)\n",
    "        \n",
    "        # Append the outlier predictions to the list\n",
    "        outlier_columns.append(outlier_series)\n",
    "    \n",
    "    # Concatenate the outlier predictions into a DataFrame\n",
    "    outlier_df = pd.concat(outlier_columns, axis=1)\n",
    "    \n",
    "    # Convert predictions to binary (1 for outliers, 0 otherwise)\n",
    "    outlier_df[outlier_df != -1] = 0\n",
    "    \n",
    "    return outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8eb7a6-c091-464e-ab35-b71be7ed05ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = detect_outliers(data, random_state=42)\n",
    "\n",
    "n_i = data.shape[0]\n",
    "\n",
    "# Due to the vast number of features, rows were removed if over 10% of their features were outliers.\n",
    "mask = pd.DataFrame(outlier_df)\n",
    "\n",
    "# Calculate the percentage of -1 values in each row\n",
    "percentage_of_minus_1 = (mask == -1).sum(axis=1) / mask.shape[1]\n",
    "\n",
    "# Filter rows where over 10% of the columns have -1\n",
    "rows_to_remove = percentage_of_minus_1 > 0.10\n",
    "\n",
    "# Remove rows from the original DataFrame\n",
    "data = data[~rows_to_remove]\n",
    "\n",
    "n_f = data.shape[0]\n",
    "\n",
    "print(f\"Removed a total of {n_i - n_f} outliers from the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecffb3bf-aec6-4da1-b237-2da2aa78b1b3",
   "metadata": {},
   "source": [
    "## Random Search (HPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c5ffb-7aa5-46ec-ae76-8936199b415c",
   "metadata": {},
   "source": [
    "This method did not work out as expected. See the following response from ChatGPT:\n",
    "\n",
    "```\n",
    "To find the total number of combinations, multiply these numbers:\n",
    "\n",
    "251 (n_estimators) * ∞ (learning_rate) * 8 (max_depth) * 20 (min_samples_split) * 10 (min_samples_leaf) * ∞ (subsample) * 3 (max_features) * ∞ (alpha) * ∞ (tol) * 2 (warm_start)\n",
    "\n",
    "However, it's important to note that learning_rate, subsample, alpha, and tol have continuous distributions, so technically, there are infinitely many possibilities within their defined ranges. Thus, the actual number of possible combinations is practically infinite for these parameters.\n",
    "\n",
    "For the discrete parameters (n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, warm_start), the total number of combinations is:\n",
    "\n",
    "251 * 8 * 20 * 10 * 3 * 2 = 1,204,800 combinations.\n",
    "\n",
    "However, considering the continuous parameters, the search space is effectively much larger, and exhaustive search across all combinations would be practically infeasible. This is one of the reasons why randomized search is preferred for hyperparameter optimization in such cases.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a0be7-1ca0-45d4-a2e3-2143637ba1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expanded parameter grid for random search\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),  # Number of boosting stages\n",
    "    'learning_rate': uniform(0.01, 0.2 - 0.01),  # Learning rate\n",
    "    'max_depth': randint(3, 10),  # Maximum depth of the individual estimators\n",
    "    'min_samples_split': randint(2, 21),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': randint(1, 11),  # Minimum number of samples required to be at a leaf node\n",
    "    'subsample': uniform(0.6, 0.4),  # Subsample ratio of the training instance\n",
    "    'max_features': [1.0, 'sqrt', 'log2'],  # Number of features to consider at each split\n",
    "    'alpha': uniform(0.0, 0.1),  # Regularization parameter\n",
    "    'tol': uniform(1e-5, 1e-3),  # Tolerance for stopping criteria\n",
    "    'warm_start': [True, False]  # Whether to reuse the solution of the previous call to fit as initialization\n",
    "}\n",
    "\n",
    "# Initialize GradientBoostingRegressor\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = GridSearchCV(gb_reg, param_distributions=param_dist, n_iter=1000, cv=5, scoring='r2', random_state=42)\n",
    "\n",
    "# Perform random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = random_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test R2 score of the best model:\", test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
